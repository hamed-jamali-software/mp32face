{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamed-jamali-software/mp32face/blob/main/coqui_XTTS_v2_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYy0F2gZIPR"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone -b dev https://github.com/camenduru/xtts2-hf\n",
        "%cd /content/xtts2-hf\n",
        "!pip install -q gradio==3.50.2 TTS==0.21.1 langid unidic-lite unidic deepspeed\n",
        "!pip install -q numpy<2.0.0 -U\n",
        "!wget https://huggingface.co/spaces/coqui/xtts/resolve/main/examples/female.wav -O /content/xtts2-hf/examples/female.wav\n",
        "!wget https://huggingface.co/spaces/coqui/xtts/resolve/main/examples/male.wav -O /content/xtts2-hf/examples/male.wav\n",
        "!wget https://huggingface.co/spaces/coqui/xtts/resolve/main/ffmpeg.zip -O /content/xtts2-hf/ffmpeg.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python app.py"
      ],
      "metadata": {
        "id": "T_f3x6IF3U4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import io, os, stat\n",
        "import subprocess\n",
        "import random\n",
        "from zipfile import ZipFile\n",
        "import uuid\n",
        "import time\n",
        "import torch\n",
        "import torchaudio\n",
        "import subprocess\n",
        "import uuid\n",
        "import time\n",
        "import torch\n",
        "import torchaudio\n",
        "import langid\n",
        "import re\n",
        "from zipfile import ZipFile\n",
        "from TTS.api import TTS\n",
        "from TTS.tts.configs.xtts_config import XttsConfig\n",
        "from TTS.tts.models.xtts import Xtts\n",
        "from TTS.utils.generic_utils import get_user_data_dir\n",
        "from huggingface_hub import HfApi\n"
      ],
      "metadata": {
        "id": "FBIbnUWp3xnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Download for mecab\n",
        "os.system('python -m unidic download')\n",
        "\n",
        "# By using XTTS you agree to CPML license https://coqui.ai/cpml\n",
        "os.environ[\"COQUI_TOS_AGREED\"] = \"1\"\n",
        "\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
        "api = HfApi(token=HF_TOKEN)\n",
        "repo_id = \"coqui/xtts\"\n",
        "\n",
        "# Use newer ffmpeg binary for Ubuntu20 to use denoising for microphone input\n",
        "print(\"Export newer ffmpeg binary for denoise filter\")\n",
        "ZipFile(\"ffmpeg.zip\").extractall()\n",
        "print(\"Make ffmpeg binary executable\")\n",
        "st = os.stat(\"ffmpeg\")\n",
        "os.chmod(\"ffmpeg\", st.st_mode | stat.S_IEXEC)\n",
        "\n",
        "# This will trigger downloading model\n",
        "print(\"Downloading if not downloaded Coqui XTTS V2\")\n",
        "from TTS.utils.manage import ModelManager\n",
        "\n",
        "model_name = \"tts_models/multilingual/multi-dataset/xtts_v2\"\n",
        "ModelManager().download_model(model_name)\n",
        "model_path = os.path.join(get_user_data_dir(\"tts\"), model_name.replace(\"/\", \"--\"))\n",
        "print(\"XTTS downloaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03ioRt7sJoNU",
        "outputId": "2a084669-e573-44f2-f5c6-bec84f051872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Export newer ffmpeg binary for denoise filter\n",
            "Make ffmpeg binary executable\n",
            "Downloading if not downloaded Coqui XTTS V2\n",
            " > Downloading model to /root/.local/share/tts/tts_models--multilingual--multi-dataset--xtts_v2\n",
            " > Model's license - CPML\n",
            " > Check https://coqui.ai/cpml.txt for more info.\n",
            "XTTS downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "config = XttsConfig()\n",
        "config.load_json(os.path.join(model_path, \"config.json\"))\n",
        "\n",
        "model = Xtts.init_from_config(config)\n",
        "model.load_checkpoint(\n",
        "    config,\n",
        "    checkpoint_path=os.path.join(model_path, \"model.pth\"),\n",
        "    vocab_path=os.path.join(model_path, \"vocab.json\"),\n",
        "    eval=True,\n",
        "    use_deepspeed=True,\n",
        ")\n",
        "model.cuda()\n",
        "\n",
        "DEVICE_ASSERT_DETECTED = 0\n",
        "DEVICE_ASSERT_PROMPT = None\n",
        "DEVICE_ASSERT_LANG = None\n",
        "\n",
        "supported_languages = config.languages\n"
      ],
      "metadata": {
        "id": "6HyR4IDBJqjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "import subprocess\n",
        "import uuid\n",
        "import time\n",
        "import re\n",
        "import datetime\n",
        "import csv\n",
        "from io import StringIO"
      ],
      "metadata": {
        "id": "ad278dN2TzT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text(text, max_length=200):\n",
        "    return textwrap.wrap(text, width=max_length, break_long_words=False)\n"
      ],
      "metadata": {
        "id": "AhUhgkQOT5LU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_speech(prompt, language, audio_file_pth=None, mic_file_path=None, use_mic=False, voice_cleanup=False, no_lang_auto_detect=False, agree=True, output_file=None):\n",
        "    if agree:\n",
        "\n",
        "\n",
        "        speaker_wav = audio_file_pth\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        try:\n",
        "            prompt = re.sub(\"([^\\x00-\\x7F]|\\w)(\\.|\\。|\\?)\", r\"\\1 \\2\\2\", prompt)\n",
        "            print(\"Generating new audio...\")\n",
        "            t0 = time.time() # زمان شروع فرآیند تولید صوت\n",
        "            out = model.inference(\n",
        "                prompt,\n",
        "                language,\n",
        "    *model.get_conditioning_latents(audio_path=speaker_wav, gpt_cond_len=30, gpt_cond_chunk_len=4, max_ref_length=60),\n",
        "                repetition_penalty=5.0,\n",
        "                temperature=0.75,\n",
        "            )\n",
        "            inference_time = time.time() - t0\n",
        "            print(f\"Time to generate audio: {round(inference_time * 1000)} milliseconds\")\n",
        "            real_time_factor = (time.time() - t0) / out['wav'].shape[-1] * 24000\n",
        "            print(f\"Real-time factor (RTF): {real_time_factor}\")\n",
        "            torchaudio.save(output_file, torch.tensor(out[\"wav\"]).unsqueeze(0), 24000)\n",
        "            print(\"Audio saved to  \"+output_file)\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            if \"device-side assert\" in str(e):\n",
        "                print(f\"Exit due to: Unrecoverable exception caused by language:{language} prompt:{prompt}\")\n",
        "                print(\"Unhandled Exception encounter, please retry in a minute\")\n",
        "                print(\"Cuda device-assert Runtime encountered need restart\")\n",
        "                if not DEVICE_ASSERT_DETECTED:\n",
        "                    DEVICE_ASSERT_DETECTED = 1\n",
        "                    DEVICE_ASSERT_PROMPT = prompt\n",
        "                    DEVICE_ASSERT_LANG = language\n",
        "\n",
        "                error_time = datetime.datetime.now().strftime(\"%d-%m-%Y-%H:%M:%S\")\n",
        "                error_data = [\n",
        "                    error_time,\n",
        "                    prompt,\n",
        "                    language,\n",
        "                    audio_file_pth,\n",
        "                    mic_file_path,\n",
        "                    use_mic,\n",
        "                    voice_cleanup,\n",
        "                    no_lang_auto_detect,\n",
        "                    agree,\n",
        "                ]\n",
        "                error_data = [str(e) if type(e) != str else e for e in error_data]\n",
        "                print(error_data)\n",
        "                write_io = StringIO()\n",
        "                csv.writer(write_io).writerows([error_data])\n",
        "                csv_upload = write_io.getvalue().encode()\n",
        "\n",
        "                filename = error_time + \"_\" + str(uuid.uuid4()) + \".csv\"\n",
        "                print(\"Writing error csv\")\n",
        "                error_api = HfApi()\n",
        "                error_api.upload_file(\n",
        "                    path_or_fileobj=csv_upload,\n",
        "                    path_in_repo=filename,\n",
        "                    repo_id=\"coqui/xtts-flagged-dataset\",\n",
        "                    repo_type=\"dataset\",\n",
        "                )\n",
        "\n",
        "                print(\"Writing error reference audio\")\n",
        "                speaker_filename = error_time + \"_reference_\" + str(uuid.uuid4()) + \".wav\"\n",
        "                error_api.upload_file(\n",
        "                    path_or_fileobj=speaker_wav,\n",
        "                    path_in_repo=speaker_filename,\n",
        "                    repo_id=\"coqui/xtts-flagged-dataset\",\n",
        "                    repo_type=\"dataset\",\n",
        "                )\n",
        "\n",
        "                space = api.get_space_runtime(repo_id=repo_id)\n",
        "                if space.stage != \"BUILDING\":\n",
        "                    api.restart_space(repo_id=repo_id)\n",
        "                else:\n",
        "                    print(\"TRIED TO RESTART but space is building\")\n",
        "            else:\n",
        "                if \"Failed to decode\" in str(e):\n",
        "                    print(\"Speaker encoding error\", str(e))\n",
        "                else:\n",
        "                    print(\"RuntimeError: non device-side assert error:\", str(e))\n",
        "                return None\n",
        "\n",
        "        return output_file\n"
      ],
      "metadata": {
        "id": "G-w6GgCEUGRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_speech(prompt, language_code, chunk_id):\n",
        "    output_file = f\"chunk_{chunk_id}.wav\"\n",
        "    language_code = \"en\"\n",
        "    audio_file_path = '/content/1.mp3'  # Path to reference audio file if needed\n",
        "    mic_file_path = None  # Path to microphone recorded audio if needed\n",
        "    use_microphone = False\n",
        "    voice_cleanup = True\n",
        "    disable_lang_auto_detect = True\n",
        "\n",
        "\n",
        "    speaker_wav = audio_file_path\n",
        "\n",
        "    # Filtering for microphone input\n",
        "    lowpassfilter = denoise = trim = loudness = True\n",
        "    lowpass_highpass = \"lowpass=8000,highpass=75,\" if lowpassfilter else \"\"\n",
        "    trim_silence = \"areverse,silenceremove=start_periods=1:start_silence=0:start_threshold=0.02,areverse,silenceremove=start_periods=1:start_silence=0:start_threshold=0.02,\" if trim else \"\"\n",
        "\n",
        "    out_filename = speaker_wav + \"filter.wav\"\n",
        "    shell_command = f\"./ffmpeg -y -i {speaker_wav} -af {lowpass_highpass}{trim_silence} {out_filename}\".split(\" \")\n",
        "    subprocess.run(shell_command, capture_output=False, text=True, check=True)\n",
        "    speaker_wav = out_filename\n",
        "\n",
        "    return text_to_speech(\n",
        "        prompt=prompt,\n",
        "        language=language_code,\n",
        "        audio_file_pth=speaker_wav,\n",
        "        mic_file_path=mic_file_path,\n",
        "        use_mic=use_microphone,\n",
        "        voice_cleanup=voice_cleanup,\n",
        "        no_lang_auto_detect=disable_lang_auto_detect,\n",
        "        agree=True,\n",
        "        output_file= output_file)"
      ],
      "metadata": {
        "id": "hOL9GLg4YSmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to combine multiple audio files into one\n",
        "def combine_audio_files(audio_files, output_file):\n",
        "\n",
        "    with open(\"file_list.txt\", \"w\") as file_list:\n",
        "      for audio_file in audio_files:\n",
        "          file_list.write(f\"file '{audio_file}'\\n\")\n",
        "\n",
        "    command = [\"ffmpeg\", \"-f\", \"concat\", \"-safe\", \"0\", \"-i\", \"file_list.txt\", \"-c\", \"copy\", output_file]\n",
        "\n",
        "    try:\n",
        "        result = subprocess.run(command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "        print(f\"Audio files successfully combined into {output_file}\")\n",
        "        print(result.stdout)\n",
        "        print(result.stderr)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error occurred during combining audio files: {e}\")\n",
        "        print(e.stdout)\n",
        "        print(e.stderr)\n",
        "\n",
        "    for audio_file in audio_files:\n",
        "      os.remove(audio_file)\n",
        "\n",
        "    # with open(output_file, 'wb') as outfile:\n",
        "    #     for audio_file in audio_files:\n",
        "    #         with open(audio_file, 'rb') as infile:\n",
        "    #             outfile.write(infile.read())\n",
        "            # Clean up temporary files\n",
        "            # os.remove(audio_file)"
      ],
      "metadata": {
        "id": "FFhNU1zhXu52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_files = [\"chunk_1.wav\", \"chunk_2.wav\",\"chunk_3.wav\"]\n",
        "output_file = \"combined_output.wav\"\n",
        "combine_audio_files(audio_files, output_file)"
      ],
      "metadata": {
        "id": "iinizpE4cZwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_text_to_speech(text, language_code):\n",
        "    # Split text into chunks of 200 characters\n",
        "    chunks = split_text(text)\n",
        "\n",
        "    all_audio_files = []\n",
        "\n",
        "\n",
        "    if os.path.isfile(\"combined_output.wav\"):\n",
        "      os.remove(\"combined_output.wav\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for idx, chunk in enumerate(chunks):\n",
        "      temp_file = generate_speech(chunk, language_code, idx + 1)\n",
        "      if os.path.exists(temp_file):\n",
        "          all_audio_files.append(temp_file)\n",
        "      else:\n",
        "          print(f\"Failed to generate audio for chunk {idx + 1}.\")\n",
        "\n",
        "    # Combine all audio files into one\n",
        "    if all_audio_files:\n",
        "        combined_audio = \"combined_output.wav\"\n",
        "        combine_audio_files(all_audio_files, combined_audio)\n",
        "        print(f\"All audio files combined into {combined_audio}\")\n",
        "    else:\n",
        "        print(\"No audio files to combine.\")"
      ],
      "metadata": {
        "id": "qn9SMN3uUVWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example usage\n",
        "prompt_text = \"\"\"Hey everyone, welcome back to the channel! Today, I want to talk about a topic that's been on my mind for a while: motivation versus discipline. We all know that motivation is great when it hits us, but there's a catch: you can’t always count on it. It tends to come at the most random times, like at 3 AM when you’re about to go back to bed but suddenly feel the urge to exercise, or in the middle of a party when you're looking at yourself in the mirror.\n",
        "\n",
        "So, what’s the answer? Discipline. It took me 18 years to figure this out. I started with motivation and watched countless videos on discipline, from big channels to smaller ones, in both Persian and English. And let me tell you, about 90% of them were pretty much useless. It wasn’t because they didn’t try, but because they tackled the subject all wrong. My goal with this video is to give you real, useful advice on how to integrate discipline into your life—no clickbait, just genuine help.\"\"\"\n",
        "\n",
        "process_text_to_speech(prompt_text, language_code=\"en\")"
      ],
      "metadata": {
        "id": "3ZUz9btqUaVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "command = [\n",
        "    \"ffmpeg\",\n",
        "    \"-y\",\n",
        "    \"-i\", \"combined_output.wav\",\n",
        "    \"-filter:a\", \"atempo=0.87\",\n",
        "    \"output_final.wav\"\n",
        "]\n",
        "\n",
        "# 2. اضافه کردن سکوت بین جملات\n",
        "command_silence = [\n",
        "    \"ffmpeg\",\n",
        "    \"-y\",  # بازنویسی خودکار فایل خروجی در صورت وجود\n",
        "    \"-i\", \"output_final.wav\",\n",
        "    \"-af\", \"aresample=async=1:min_hard_comp=0.100000:first_pts=0,apad=pad_dur=2:pad_len=2\",\n",
        "    \"output_with_silence.wav\"\n",
        "]\n",
        "\n",
        "# اجرای فرمان FFmpeg\n",
        "try:\n",
        "    result = subprocess.run(command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "    print(\"Command executed successfully.\")\n",
        "    result_silence = subprocess.run(command_silence, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "\n",
        "\n",
        "            # Filtering for microphone input\n",
        "    denoise = trim = loudness = True\n",
        "    lowpass_highpass = \"lowpass=8000,highpass=75,\"\n",
        "    trim_silence = \"areverse,silenceremove=start_periods=1:start_silence=0:start_threshold=0.02,areverse,silenceremove=start_periods=1:start_silence=0:start_threshold=0.02,\" if trim else \"\"\n",
        "\n",
        "    out_filename = \"output_with_silence_end.wav\"\n",
        "    speaker_wav = \"output_with_silence.wav\"\n",
        "    shell_command = f\"./ffmpeg -y -i {speaker_wav} -af {lowpass_highpass}{trim_silence} {out_filename}\".split(\" \")\n",
        "    subprocess.run(shell_command, capture_output=False, text=True, check=True)\n",
        "    speaker_wav = out_filename\n",
        "    print(\"Filtered microphone input\")\n",
        "\n",
        "\n",
        "    print(result.stdout)\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(\"Error occurred:\")\n",
        "    print(e.stderr)"
      ],
      "metadata": {
        "id": "Crue82UIiweh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}